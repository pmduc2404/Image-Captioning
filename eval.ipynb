{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from models import Encoder, Decoder\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./flickr8k/captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lowercase=False, remove_punc=False, remove_num=False, sos_token='<sos>', eos_token='<eos>'):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if remove_punc:\n",
    "        text = ''.join([ch for ch in text if ch not in punctuation])\n",
    "    if remove_num:\n",
    "        text = ''.join([ch for ch in text if ch not in '1234567890'])\n",
    "    text = [sos_token] + word_tokenize(text) + [eos_token]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'a', 'cat', 'is', 'sitting', 'on', 'the', 'table', '<eos>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"A cat is sitting on the table.\", lowercase=True, remove_punc=True, remove_num=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cap = data['caption'].apply(lambda x: clean_text(x, lowercase=True, remove_punc=True, remove_num=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_caption'] = clean_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(clean_cap, specials=[unk_token, pad_token, sos_token, eos_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_idx = vocab[pad_token]\n",
    "unk_token_idx = vocab[unk_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to number\n",
    "def text_to_number(text, vocab):\n",
    "    return [vocab[token] for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int = clean_cap.apply(lambda x: text_to_number(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embed_caption'] = to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>clean_caption</th>\n",
       "      <th>embed_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2973269132_252bfd0160.jpg</td>\n",
       "      <td>A large wild cat is pursuing a horse across a ...</td>\n",
       "      <td>[&lt;sos&gt;, a, large, wild, cat, is, pursuing, a, ...</td>\n",
       "      <td>[2, 4, 56, 1693, 584, 8, 4821, 4, 229, 125, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270263570_3160f360d3.jpg</td>\n",
       "      <td>Two brown dogs fight on the leafy ground .</td>\n",
       "      <td>[&lt;sos&gt;, two, brown, dogs, fight, on, the, leaf...</td>\n",
       "      <td>[2, 14, 28, 32, 517, 7, 6, 1525, 170, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2053006423_6adf69ca67.jpg</td>\n",
       "      <td>A man in shorts is standing on a rock looking ...</td>\n",
       "      <td>[&lt;sos&gt;, a, man, in, shorts, is, standing, on, ...</td>\n",
       "      <td>[2, 4, 12, 5, 161, 8, 39, 7, 4, 85, 89, 84, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512101751_05a6d93e19.jpg</td>\n",
       "      <td>a muzzled white dog is running on the grass .</td>\n",
       "      <td>[&lt;sos&gt;, a, muzzled, white, dog, is, running, o...</td>\n",
       "      <td>[2, 4, 900, 15, 10, 8, 33, 7, 6, 42, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3156406419_38fbd52007.jpg</td>\n",
       "      <td>A person skiing downhill .</td>\n",
       "      <td>[&lt;sos&gt;, a, person, skiing, downhill, &lt;eos&gt;]</td>\n",
       "      <td>[2, 4, 44, 377, 709, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  2973269132_252bfd0160.jpg   \n",
       "1   270263570_3160f360d3.jpg   \n",
       "2  2053006423_6adf69ca67.jpg   \n",
       "3   512101751_05a6d93e19.jpg   \n",
       "4  3156406419_38fbd52007.jpg   \n",
       "\n",
       "                                             caption  \\\n",
       "0  A large wild cat is pursuing a horse across a ...   \n",
       "1         Two brown dogs fight on the leafy ground .   \n",
       "2  A man in shorts is standing on a rock looking ...   \n",
       "3      a muzzled white dog is running on the grass .   \n",
       "4                         A person skiing downhill .   \n",
       "\n",
       "                                       clean_caption  \\\n",
       "0  [<sos>, a, large, wild, cat, is, pursuing, a, ...   \n",
       "1  [<sos>, two, brown, dogs, fight, on, the, leaf...   \n",
       "2  [<sos>, a, man, in, shorts, is, standing, on, ...   \n",
       "3  [<sos>, a, muzzled, white, dog, is, running, o...   \n",
       "4        [<sos>, a, person, skiing, downhill, <eos>]   \n",
       "\n",
       "                                       embed_caption  \n",
       "0  [2, 4, 56, 1693, 584, 8, 4821, 4, 229, 125, 4,...  \n",
       "1           [2, 14, 28, 32, 517, 7, 6, 1525, 170, 3]  \n",
       "2  [2, 4, 12, 5, 161, 8, 39, 7, 4, 85, 89, 84, 23...  \n",
       "3            [2, 4, 900, 15, 10, 8, 33, 7, 6, 42, 3]  \n",
       "4                            [2, 4, 44, 377, 709, 3]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # data type convert to tensor\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "num_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_dim, dropout)\n",
    "model_load = Decoder(embed_dim, hidden_dim, vocab_size, num_layers, device, encoder, dropout )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.load_state_dict(torch.load('best-model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image, vocab, max_length=30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(image).unsqueeze(1)\n",
    "        input = features\n",
    "        hidden = torch.zeros(model.num_layers, 1, model.lstm.hidden_size).to(model.device)\n",
    "        cell = torch.zeros(model.num_layers, 1, model.lstm.hidden_size).to(model.device)\n",
    "\n",
    "        caption = []\n",
    "        for _ in range(max_length):\n",
    "            output, (hidden, cell) = model.lstm(input, (hidden, cell))\n",
    "            output = model.linear(output.squeeze(1))\n",
    "            predicted = output.argmax(1)\n",
    "            caption.append(predicted.item())\n",
    "            input = model.dropout(model.embed(predicted)).unsqueeze(1)\n",
    "            if predicted.item() == vocab['<eos>']:\n",
    "                break\n",
    "    pred = vocab.lookup_tokens(caption)      \n",
    "    return ' '.join(pred[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test[['image', 'caption', 'clean_caption']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2973269132_252bfd0160.jpg',\n",
       "        'A large wild cat is pursuing a horse across a meadow .',\n",
       "        list(['<sos>', 'a', 'large', 'wild', 'cat', 'is', 'pursuing', 'a', 'horse', 'across', 'a', 'meadow', '<eos>'])],\n",
       "       ['270263570_3160f360d3.jpg',\n",
       "        'Two brown dogs fight on the leafy ground .',\n",
       "        list(['<sos>', 'two', 'brown', 'dogs', 'fight', 'on', 'the', 'leafy', 'ground', '<eos>'])],\n",
       "       ['2053006423_6adf69ca67.jpg',\n",
       "        'A man in shorts is standing on a rock looking out at the view from the hilltop .',\n",
       "        list(['<sos>', 'a', 'man', 'in', 'shorts', 'is', 'standing', 'on', 'a', 'rock', 'looking', 'out', 'at', 'the', 'view', 'from', 'the', 'hilltop', '<eos>'])],\n",
       "       ...,\n",
       "       ['2848895544_6d06210e9d.jpg',\n",
       "        'Two little boys in uniforms play soccer .',\n",
       "        list(['<sos>', 'two', 'little', 'boys', 'in', 'uniforms', 'play', 'soccer', '<eos>'])],\n",
       "       ['431410325_f4916b5460.jpg',\n",
       "        'A wet brown dog is leaving the water .',\n",
       "        list(['<sos>', 'a', 'wet', 'brown', 'dog', 'is', 'leaving', 'the', 'water', '<eos>'])],\n",
       "       ['3569284680_44fef444ef.jpg',\n",
       "        'The boy in the red top is following the girl in the colorful dress up a flight of stairs .',\n",
       "        list(['<sos>', 'the', 'boy', 'in', 'the', 'red', 'top', 'is', 'following', 'the', 'girl', 'in', 'the', 'colorful', 'dress', 'up', 'a', 'flight', 'of', 'stairs', '<eos>'])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(model, data, vocab, device):\n",
    "    captioning_corpus = []\n",
    "    reference_corpus = []\n",
    "    for img, caption, clean_cap in data:\n",
    "        img = Image.open(f\"./flickr8k/Images/{img}\")\n",
    "        img = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        captioned = predict_caption(model, img, vocab)\n",
    "        \n",
    "        captioning_corpus.append(captioned.split())\n",
    "        reference_corpus.append([clean_cap[1:-1]])\n",
    "    \n",
    "    return corpus_bleu(reference_corpus, captioning_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04511407184799061"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu_score(model_load, test_data, vocab, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
